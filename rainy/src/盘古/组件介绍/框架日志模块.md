# 框架日志模块

> #### 如何使用

业务端引入如下模块

**xml**

```
<dependency>
  <groupId>com.kingtsoft.pangu</groupId>
  <artifactId>pangu-frame-log</artifactId>
  <version>${pangu.version}</version>
</dependency>
```

```
配置文件
    日志依赖于kafka，所以需要进行kafka相关配置，附加可选的配置详见pangu kafka配置
配置文件主要如下：
    pangu.log.enabled  整体日志的开关
    pangu.log.async  是否异步开启
    pangu.log.module  日志记录的模块
    pangu.log.persistent-dir  日志本地持久化存储目录，默认{user.home}/pangu/msg-local
    pangu.log.db-log.enabled  是否开启持久层日志
    pangu.log.req-log.enabled  是否开启请求日志
```

**yaml**

```
spring:
kafka:
bootstrap-servers: 10.1.50.131:9092
pangu:
kafka:
servers: ${spring.kafka.bootstrap-servers}
log:
# 是否开启拓展日志采集
enabled: true
# 异步
async: true
# 日志模块
module: pangu
# 日志本地化存储路径
persistent-dir:
db-log:
# 持久层监控日志
enabled: true
req-log:
     	# 请求监控日志
enabled: true
```

```
业务代码
```

**java**

```
privatestaticfinal PgLogger LOGGER = PanguLoggerFactory.getLogger(TestService.class);


@PanguLog(operateType= PanguLogTypeEnum.TYPE_OTHER, operateName="测试", inParam=true, outParam=true)
public Object testLog(String inp) {
    OisRegSchedulePool oisRegSchedulePool =newOisRegSchedulePool();
    oisRegSchedulePool.setState(1);
    oisRegSchedulePool.setPoolCode(UUID.randomUUID().toString().substring(0, 8));
    oisRegSchedulePool.setScheduleSn(1L);
    LOGGER.info("test1{}", oisRegSchedulePool);

int i = oisRegSchedulePoolMapper.insert(oisRegSchedulePool);
    System.out.println(i);
    oisRegSchedulePool.setPoolSn(112L);
    oisRegSchedulePoolMapper.updateAuto(oisRegSchedulePool);

    oisRegSchedulePoolMapper.deleteById(1L);

return"abcdefg";
}
```

**注解日志**

```
@PanguLog，方法下添加如下所示注解
注解属性如下：
    objId  关键对象ID可空
    operateName  操作名称
    logLevel  日志级别 1-ALL、2-TRACE、3-DEBUG、4-INFO、5-WARN、6-ERROR、7-FATAL、8-OFF
    operateType  操作类型1-增、2-删、3-改、9-其他（比如系统日志记录）
    inParam    入参记录默认false;
    outParam  出参记录默认false;
    typeExPress  也是操作类型优先级高于 operateType;
```

**持久层日志**

```
    配置文件添加pangu.log.db-log.enabled=true即可, 会记录新增、更新及删除操作
```

**常规日志模式**

```
    private static final PgLogger LOGGER = PanguLoggerFactory.getLogger(TestService.class);
    执行LOGGER.info(“test1{}”, oisRegSchedulePool);记录即可
而AllLoggers.APPLICATION.info("AllLoggers.APPLICATION.info");也是一种方
式，不过此方式要配合filebeat进行采集，这里不做过多介绍。
```

**server端**

```
    需要监听业务发送的消息，所以只需要配置文件配置
```

**yaml**

```
spring:
kafka:
bootstrap-servers: 10.1.50.131:9092
pangu:
kafka:
servers: ${spring.kafka.bootstrap-servers}
pangu:
elasticsearch:
# ES地址
hosts: '10.1.50.63:9200'
```

> #### 技术原理

**注解**

```
    注解的方式主要定义了一个增强类PanguLogAspect 切入口为带有PanguLog注解的方法
操作名，操作类型，对象ID的配置支持spel表达式，异步标记isAsync可以线程化执行以减小对
业务的影响。
```

**java**

```
package com.kingtsoft.pangu.frame.log;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang.com <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Slf4j
@Aspect
@Component
public class  PanguLogAspectimplementsEnvironmentAware {

privateboolean   isAsync;

private final   SpelExpressionParser parser =newSpelExpressionParser();

private final   DefaultParameterNameDiscoverer nameDiscoverer =newDefaultParameterNameDiscoverer();

    @Autowired(required=false)
private LogHandler logHandler;

private final   ExecutorService pool = LogThreadPool.LOG_POOL;

private final   KafkaTemplate<String, Object> kafkaTemplate;

private final   DataToLocalHandler dataToLocalHandler;

private final   PanguLogProperties panguLogProperties;

publicPanguLogAspect(KafkaTemplate<String, Object> kafkaTemplate,
                          DataToLocalHandler dataToLocalHandler,
                          PanguLogProperties panguLogProperties) {
this.kafkaTemplate = kafkaTemplate;
this.dataToLocalHandler = dataToLocalHandler;
this.panguLogProperties = panguLogProperties;
    }

    @Override
private void   setEnvironment(Environment environment) {
        String isAsync = environment.getProperty("pangu.log.async");
if (isAsync !=null) {
this.isAsync = boolean  .parseboolean  (isAsync);
        }
    }

    @Pointcut("@annotation(com.kingtsoft.pangu.frame.log.annotation.PanguLog)")
private void   logPointCut() {
    }

    /**
     * 日志保存切口
     *
     * @paramjoinPoint 切口入参
     * @return 接口执行返回数据
     * @author 金炀
     */
    @Around("logPointCut()")
public Object saveOperation(ProceedingJoinPoint joinPoint) throws Throwable {
        Object ret = joinPoint.proceed();
try {
if (RequestContextHolder.getRequestAttributes() !=null) {
                HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();
                String host = request.getRemoteHost();

Object[] args = joinPoint.getArgs();
                List<Object> agentArgs =new ArrayList<>();
if (args !=null&& args.length >0) {
                    Collections.addAll(agentArgs, args);
                }

// 全局跟踪ID
                String traceId = LogTraceHolder.getTraceIdNecessary();

if (this.isAsync) {
                    pool.execute(() ->doOperationLogSave(joinPoint, agentArgs.toArray(), host, ret, traceId));
                } else {
doOperationLogSave(joinPoint, agentArgs.toArray(), host, ret, traceId);
                }
            }
        } catch (Exception e) {
            log.error(e.getMessage());
        }

return ret;
    }

    /**
     * 日志相关数据封装
     *
     * @paramjoinPoint 日志数据
     * @author 金炀
     */
private void  doOperationLogSave(ProceedingJoinPoint joinPoint, Object[] args, String host, Object outParam, String traceId) {
        MethodSignature signature = (MethodSignature) joinPoint.getSignature();
        Method method = signature.getMethod();

        PanguLog panguLog = method.getAnnotation(PanguLog.class);

        String operateName = panguLog.operateName();
// SPEL支持
if (operateName.contains("#")) {
            operateName =generateKeyBySpEl(operateName, joinPoint, args);
        }

        Integer operateType =getOperateType(panguLog, joinPoint, args);

        String objId = panguLog.objId();
if (StringUtils.hasText(objId) && objId.contains("#")) {
            objId =generateKeyBySpEl(objId, joinPoint, args);
        }

if (!StringUtils.hasText(operateName)) {
            Integer finalOperateType = operateType;
            Optional<PanguLogTypeEnum> logTypeEnum = Arrays.stream(PanguLogTypeEnum.values()).filter(
                    en -> en.getLogType().equals(finalOperateType)
            ).findAny();

if (logTypeEnum.isEmpty()) {
                operateType =9;
            } else {
                operateName += logTypeEnum.get().getMsg();
            }
        }

        String context = ("source："+ host +" 方法："+ method.getName() +"【"+ operateName +"】");

        StringBuilder operateContent =newStringBuilder(context +"->");

Object[] params = joinPoint.getArgs();
if (params !=null) {
for (Object obj : params) {
                operateContent.append(obj.getClass()).append(";");
            }
        }

//...存储
    }

private void  sendMsg(LogOperateMessage logOperateMessage) {
        Map<String, Object> map =new HashMap<>(4);
        map.put(KafkaHeaders.TOPIC, FrameLogConst.LOG_TOPIC_ANNOTATION);
        map.put(KafkaHeaders.KEY, PanguLogUtil.createMsgKey(FrameLogConst.LogType.ANNOTATION_LOG));
        map.put(FrameLogConst.LOG_TYPE_KEY, FrameLogConst.LogType.ANNOTATION_LOG);
try {
            Message<String> message =new GenericMessage<>(JSON.toJSONString(logOperateMessage), newMessageHeaders(map));
            kafkaTemplate.send(message);
        } catch (Exception e) {
            e.printStackTrace();
            log.error("消息发送失败！");
// 发送失败持久化
            dataToLocalHandler.doDataToLocal(JSON.toJSONString(logOperateMessage), map, FrameLogConst.LogType.ANNOTATION_LOG);
        }
    }

private Integer getOperateType(PanguLog panguLog, ProceedingJoinPoint joinPoint, Object[] args) {
        Integer operateType;

        String typeExPress = panguLog.typeExPress();
if (StringUtils.hasText(typeExPress) && typeExPress.contains("#")) {
            operateType =generateKeyBySpEl(typeExPress, joinPoint, args);
        } else {
            PanguLogTypeEnum operateTypeEnum = panguLog.operateType();
            operateType = operateTypeEnum.getLogType();
        }

return operateType;
    }

    @SuppressWarnings({"unchecked"})
private <T> T generateKeyBySpEl(String spElString, ProceedingJoinPoint joinPoint, Object[] args) {
        MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature();
String[] paramNames = nameDiscoverer.getParameterNames(methodSignature.getMethod());
        Expression expression = parser.parseExpression(spElString);
        EvaluationContext context =newStandardEvaluationContext();
for (int i =0; i < args.length; i++) {
assert paramNames !=null;
            context.setVariable(paramNames[i], args[i]);
        }
return (T) expression.getValue(context);
    }
}
```

```
    注意，消息发送的时候，异常捕获有个
dataToLocalHandler.doDataToLocal(JSON.toJSONString(logOperateMessage), map, FrameLogConst.LogType.ANNOTATION_LOG);
此方法是将消息数据持久化到本地,防止消息丢失。部分代码如下。不难发现目录会根据日期进行创建，
并在最后会调用modifyPathDir方法，此方法会整理历史目录数据，若存在历史目录数据，为了节省
空间，会对所有历史目录进行压缩。
```

**java**

```
@SneakyThrows
private void   doDataToLocal(String data, Map<String, Object> messageHeader, String logType) {
if (!StringUtils.hasText(logType)) {
            logType = FrameLogConst.LogType.UN_KNOW_LOG;
        }
        LocalDate nowDate = LocalDate.now();
        String logPath = String.format("%s/%s/%s", basePath, nowDate, logType);
        File baseDir =newFile(logPath);
if (!baseDir.exists()) {
if (!baseDir.mkdirs()) {
thrownewTipException("消息持久化基础目录创建失败！");
            }
        }

        String key = (String) messageHeader.get(KafkaHeaders.KEY);

        File jsonFile =newFile(logPath +"/"+ key +".json");
if (!jsonFile.exists()) {
if (!jsonFile.createNewFile()) {
thrownewTipException("消息持久化文件创建失败！");
            }
        }
// 在考虑要不要加个上传计数器，用来统计上传次数(失败次数)
        JSONObject jsonObject =newJSONObject()
                .fluentPut(FrameLogConst.MESSAGE_HEADER_KEY, messageHeader)
                .fluentPut(FrameLogConst.MESSAGE_DATA_KEY, data);

// 将格式化后的字符串写入文件
        Writer write =null;
try {
            write =newOutputStreamWriter(newFileOutputStream(jsonFile), StandardCharsets.UTF_8);
            write.write(jsonObject.toJSONString());
        } finally {
if (write !=null) {
                write.flush();
                write.close();
            }
        }

modifyPathDir();
    }

private void  modifyPathDir() {
        File baseDir =newFile(basePath);
if (!baseDir.exists()) {
return;
        }

File[] fileDates = baseDir.listFiles();
if (fileDates ==null) {
return;
        }

        String nowDateStr = LocalDate.now().toString();

for (File fileDate : fileDates) {
if (!fileDate.isDirectory()) {
continue;
            }

if (fileDate.getName().equals(nowDateStr) || fileDate.getName().equals(FrameLogConst.LOG_DIR_TMP)) {
continue;
            }

            CompressUtil.compressToTgz(fileDate.toPath().toString());
clearSource(fileDate);
        }
    }
```

```
    当然也暴露了一个接口用来对数据进行补传。会对数据进行解压，并且一份份上传，若只上传了
一部分，又会对未上传部分重新压缩。确保数据处理精细的颗粒度。
```

**持久层**

```
    持久层的日志自动化记录主要用了mybatis的拦截器MbFrameLogIntercept
    （目前只接入了此持久层框架）
    只记录增删改操作，定义了持久层特有的数据结构DbOperateLog.然后解析SQL
数据，持久层因为操作的频繁性，所以强制是多线程处理的消息。getStackTrace并且
可以看出会尽可能定位执行的代码段。
```

**java**

```
private void  doExProceed(Object[] args, LocalDateTime now, MappedStatement ms, OperateContext operateContext) {
    DbOperateLog dbOperateLog =newDbOperateLog()
            .setCreateTime(now)
            .setLogPosition(getStackTrace(ms.getId(), "com.sun.proxy"))
            .setOperateContent(String.format("【数据库%s日志】 值：%s",
                    (ms.getSqlCommandType().equals(SqlCommandType.INSERT)) ?"新增":
                            ms.getSqlCommandType().equals(SqlCommandType.DELETE) ?"删除":"更新",
                    JSON.toJSONString(args[1]))
            );
    BoundSql boundSql = ms.getBoundSql(args[1]);
    String sql =getSql(boundSql, ms);
    String tableName =getTableName(sql, ms.getSqlCommandType());
    dbOperateLog.setExecSql(sql).setTableName(tableName);
if (operateContext !=null) {
        BeanUtils.copyProperties(operateContext, dbOperateLog);
    }
    dbOperateLog.setModule(panguLogProperties.getModule());
    dbOperateLog.setTraceId(LogTraceHolder.getTraceIdNecessary());
    dbOperateLog.setIp(LogTraceHolder.getLogLocalIp());

    pool.execute(() ->sendMsg(dbOperateLog));
}
```

```
    数据结构OperateContext为记录基础数据，主要由客户端传入
```

**java**

```
public class  DbOperateLogextendsOperateContextimplementsSerializable {

    /**
     * 记录ID
     */
private long  id;

    /**
     * 所属模块
     */
private String module;

    /**
     * 操作内容
     */
private String operateContent;

    /**
     * 执行时间
     */
    @JsonFormat(pattern="yyyy-MM-dd HH:mm:ss", timezone="GMT+8")
private LocalDateTime createTime;

    /**
     * 执行时间(时间戳)
     */
private long  createTimelong ;

    /**
     * 表名
     */
private String tableName;

    /**
     * 执行的SQL
     */
private String execSql;

    /**
     * 执行的代码位置
     */
private String logPosition;

    /**
     * ip地址
     */
private String ip;

    /**
     * 跟踪ID
     */
private String traceId;

    /**
     * 备注
     */
private String remark;
}
```

**java**

```
@Data
public class  OperateContextimplementsSerializable {

    /** 菜单代码 */
private String menuCode;

    /** 菜单名称 */
private String menuName;

    /** 客户端 */
private String terminal;

    /** 用户ID */
private Integer userId;
}
```

**常规**

```
    常规模式指的类似于log4j那样直接手动记录日志，主要运用了jdk动态代理实现。
定义了一个基础接口，可以看出主要是集成了SLF4J的门面接口，用来定义实际执行的方法。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

import org.slf4j.Logger;
import org.slf4j.Marker;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
public interface  PgLoggerextendsLogger {

    Class<?> getClazz();

    @Override
voidtrace(String s);

    @Override
voidtrace(String s, Object o);

    @Override
voidtrace(String s, Object o, Object o1);

    @Override
voidtrace(String s, Object... objects);

    @Override
voidtrace(String s, Throwable throwable);

    @Override
voidtrace(Marker marker, String s);

    @Override
voidtrace(Marker marker, String s, Object o);

    @Override
voidtrace(Marker marker, String s, Object o, Object o1);

    @Override
voidtrace(Marker marker, String s, Object... objects);

    @Override
voidtrace(Marker marker, String s, Throwable throwable);

    @Override
voiddebug(String s);

    @Override
voiddebug(String s, Object o);

    @Override
voiddebug(String s, Object o, Object o1);

    @Override
voiddebug(String s, Object... objects);

    @Override
voiddebug(String s, Throwable throwable);

    @Override
voiddebug(Marker marker, String s);

    @Override
voiddebug(Marker marker, String s, Object o);

    @Override
voiddebug(Marker marker, String s, Object o, Object o1);

	// 等等
}
```

```
    PgLoggerImpl 为PgLogger的实现类，其实就是实现了slf4j的日志接口方法。
目的一个是为了后续拓展及添加切入口。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.Marker;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
public class  PgLoggerImplimplementsPgLogger {

private final   Logger log;

private final   Class<?> clazz;

publicPgLoggerImpl(Class<?> clazz) {
this.clazz = clazz;
        log = LoggerFactory.getLogger(clazz);
    }

    @Override
public class  <?> getClazz() {
return clazz;
    }

    @Override
public String getName() {
return log.getName();
    }

    @Override
publicboolean  isTraceEnabled() {
return log.isTraceEnabled();
    }

    @Override
private void   trace(String s) {
        log.trace(s);
    }

    @Override
private void   trace(String s, Object o) {
        log.trace(s, o);
    }

    @Override
private void   trace(String s, Object o, Object o1) {
        log.trace(s, o, o1);
    }

    @Override
private void   trace(String s, Object... objects) {
        log.trace(s, objects);
    }

    @Override
private void   trace(String s, Throwable throwable) {
        log.trace(s, throwable);
    }

    @Override
publicboolean  isTraceEnabled(Marker marker) {
return log.isTraceEnabled(marker);
    }

    @Override
private void   trace(Marker marker, String s) {
        log.trace(marker, s);
    }

    @Override
private void   trace(Marker marker, String s, Object o) {
        log.trace(marker, s, o);
    }

    @Override
private void   trace(Marker marker, String s, Object o, Object o1) {
        log.trace(marker, s, o, o1);
    }

    @Override
private void   trace(Marker marker, String s, Object... objects) {
        log.trace(marker, s, objects);
    }

    @Override
private void   trace(Marker marker, String s, Throwable throwable) {
        log.trace(marker, s, throwable);
    }

    @Override
publicboolean  isDebugEnabled() {
return log.isDebugEnabled();
    }

// 等等

}
```

```
    然后定义一个门面接口生成的PanguLoggerFactory，内部静态方法getLogger会
创建一个PgLogger的代理，代理为类NormalLogProxy，主要是获取NormalLogInterceptor，
可以发现NormalLogInterceptor对象是通过NormalBeanCache缓存的，若不存在则会
新构建一个，保证了此类的单例性。然后创建了类NormalLogInvocationHandler。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

import java.lang.reflect.Proxy;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
public class  PanguLoggerFactory {

publicstatic PgLogger getLogger(Class<?> clazz) {
        PgLogger target =newPgLoggerImpl(clazz);
        NormalLogProxy logProxy =newNormalLogProxy();
return (PgLogger) logProxy.getProxy(target);
    }
}

classNormalLogProxy {
public Object getProxy(Object object) {
        NormalLogInterceptor interceptor = NormalBeanCache.interceptor;
if (interceptor ==null) {
            interceptor =newNormalLogInterceptor();
            NormalBeanCache.interceptor = interceptor;
        }

        NormalLogInvocationHandler normalHandler =newNormalLogInvocationHandler(interceptor);
        normalHandler.setObject(object);
return Proxy.newProxyInstance(object.getClass().getClassLoader(),
                object.getClass().getInterfaces(), normalHandler);
    }
}
```

```
    查看NormalLogInvocationHandler类如下，他根据代理规范，实现了InvocationHandler接口，
对象及拦截器都会通过构造器注入（手动赋值，非spring），然后配置执行器为实际方法执行后执行
NormalLogInterceptor的afterNormalLog方法。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
public class  NormalLogInvocationHandlerimplementsInvocationHandler {

private Object object;

private final   NormalLogInterceptor interceptor;

publicNormalLogInvocationHandler(NormalLogInterceptor interceptor) {
this.interceptor = interceptor;
    }

private void   setObject(Object object) {
this.object = object;
    }

    @Override
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Object result = method.invoke(object, args);
        interceptor.afterNormalLog(proxy, method, args);
return result;
    }
}
```

```
    继续查看NormalLogInterceptor类，可以发现NormalLogInterceptor并非
被spring所托管，因为主体的那个类是通过new的形式产生了，所以内部的对象都要通过NormalBeanCache
中缓存的对象来处理。NormalLogInterceptor本身就是针对log日志的结构体，对传入信息进行解析，
包括支持{}等模式复制及异常对象基类及Marker格式的解析。这样就可以组装好结构化数据进行传输了。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Slf4j
public class  NormalLogInterceptor {

private KafkaTemplate<String, Object> kafkaTemplate;

private DataToLocalHandler dataToLocalHandler;

private PanguLogProperties panguLogProperties;

private final   ExecutorService pool = LogThreadPool.LOG_POOL;

public NormalLogInterceptor setKafkaTemplate(KafkaTemplate<String, Object> kafkaTemplate) {
this.kafkaTemplate = kafkaTemplate;
returnthis;
    }

public NormalLogInterceptor setDataToLocalHandler(DataToLocalHandler dataToLocalHandler) {
this.dataToLocalHandler = dataToLocalHandler;
returnthis;
    }

public NormalLogInterceptor setPanguLogProperties(PanguLogProperties panguLogProperties) {
this.panguLogProperties = panguLogProperties;
returnthis;
    }

private void   afterNormalLog(Object proxy, Method method, Object[] args) {
if (method.getName().startsWith("is") || method.getName().startsWith("get")) {
return;
        }

        Class<?> aClass;
try {
            Method methodClazz = method.getDeclaringClass().getMethod("getClazz");
            aClass = (Class<?>) methodClazz.invoke(proxy);
        } catch (IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {
            e.printStackTrace();
return;
        }
// 开始消息传输
        LogOperateMessage logOperateMessage =newLogOperateMessage();
        logOperateMessage.setLogPosition(getStackTrace("com.sun.proxy"));
        logOperateMessage.setClassName(aClass.getName());
if (args[0] instanceof Marker) {
            Marker marker = (Marker) args[0];
            logOperateMessage.setOperateName(marker.getName());
        }
        logOperateMessage.setOperateContent(getLogContent(method, args, 0));
        logOperateMessage.setCreateTime(LocalDateTime.now());
        logOperateMessage.setLogLevel(getLogLevel(method.getName()));
        logOperateMessage.setModule(panguLogProperties.getModule());
        logOperateMessage.setTraceId(LogTraceHolder.getTraceIdNecessary());
        logOperateMessage.setIp(LogTraceHolder.getLogLocalIp());
        pool.execute(() ->sendMsg(logOperateMessage));
    }

private String getLogContent(Method method, Object[] args, intstartIdx) {
        Class<?> [] classes = method.getParameterTypes();

if (classes.length - startIdx ==1) {
return args[0].toString();
        }

if (args[0] instanceof Marker) {
Object[] params =newObject[args.length -1];
            System.arraycopy(args, 1, params, 0, args.length -1);
returngetLogContent(method, params, 1);
        }

if (classes.length - startIdx >=2) {
if (args[1] instanceof Throwable) {
returngetLogStringContent(args);
            } elseif ("[Ljava.lang.Object;".equals(args[1].getClass().getName())) {
Object[] params = (Object[]) args[1];
returngetLogStringValueContent((String) args[0], params);
            } else {
returngetLogStringValueContent(args);
            }
        }

return"";
    }

private String getLogStringValueContent(Object[] params) {
        String text = (String) params[0];

for (int i =1; i < params.length; i++) {
            text = text.replace("{}", params[i].toString());
        }

return text;
    }

private String getLogStringValueContent(String text, Object[] params) {
for (Object param : params) {
            text = text.replace("{}", param.toString());
        }

return text;
    }

private String getLogStringContent(Object[] args) {
        StringBuilder text =newStringBuilder((String) args[0]).append("\n");
        Throwable throwable = (Throwable) args[1];
        text.append(throwable.toString()).append("\n");
StackTraceElement[] traceElements = throwable.getStackTrace();
for (StackTraceElement traceElement : traceElements) {
            text.append(traceElement.toString()).append("\n");
        }

return text.toString();
    }

private Integer getLogLevel(String methodName) {
for (PanguLogLevelEnum levelEnum : PanguLogLevelEnum.values()) {
if (levelEnum.getName().toLowerCase().equals(methodName)) {
return levelEnum.getLogLevel();
            }
        }

return PanguLogLevelEnum.LEVEL_INFO.getLogLevel();
    }

publicstatic String getStackTrace(String className) {
StackTraceElement[] stackTrace = Thread.currentThread().getStackTrace();
for (int i =0; i < stackTrace.length; i++) {
            StackTraceElement traceElement = stackTrace[i];
if (traceElement.getClassName().contains(className) && i < stackTrace.length -1) {
return stackTrace[i +1].toString();
            }
        }

returnnull;
    }

private void  sendMsg(LogOperateMessage logOperateMessage) {
        Map<String, Object> map =new HashMap<>(4);
        map.put(KafkaHeaders.TOPIC, FrameLogConst.LOG_TOPIC_NORMAL);
        map.put(KafkaHeaders.KEY, PanguLogUtil.createMsgKey(FrameLogConst.LogType.NORMAL_LOG));
        map.put(FrameLogConst.LOG_TYPE_KEY, FrameLogConst.LogType.NORMAL_LOG);

try {
            Message<String> message =new GenericMessage<>(JSON.toJSONString(logOperateMessage), newMessageHeaders(map));
            kafkaTemplate.send(message);
        } catch (Exception e) {
            e.printStackTrace();
            log.error("消息发送失败！");
            dataToLocalHandler.doDataToLocal(JSON.toJSONString(logOperateMessage), map, FrameLogConst.LogType.NORMAL_LOG);
        }

    }
}
```

```
    NormalBeanCache数据来源于下main这个类，在应用启动时会获取各类所需的bean，
并且缓存在NormalBeanCache.interceptor中，这样就可以做到在new一个日志对象时候运用到bean对象。
```

**java**

```
package com.kingtsoft.pangu.frame.log.normal;

import com.kingtsoft.pangu.frame.log.DataToLocalHandler;
import com.kingtsoft.pangu.frame.log.PanguLogProperties;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.KafkaTemplate;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Slf4j
@Configuration
public class  NormalLogConfigurationimplementsApplicationRunner {

private final   KafkaTemplate<String, Object> kafkaTemplate;

private final   DataToLocalHandler dataToLocalHandler;

private final   PanguLogProperties panguLogProperties;

publicNormalLogConfiguration(KafkaTemplate<String, Object> kafkaTemplate,
                                  DataToLocalHandler dataToLocalHandler,
                                  PanguLogProperties panguLogProperties) {
this.kafkaTemplate = kafkaTemplate;
this.dataToLocalHandler = dataToLocalHandler;
this.panguLogProperties = panguLogProperties;
    }

    @Override
private void   run(ApplicationArguments args) {
if (NormalBeanCache.interceptor ==null) {
            NormalBeanCache.interceptor =newNormalLogInterceptor();
        }
        NormalBeanCache.interceptor
                .setKafkaTemplate(kafkaTemplate)
                .setDataToLocalHandler(dataToLocalHandler)
                .setPanguLogProperties(panguLogProperties);
    }
}
```

**server端**

```
    日志的服务端主要是接收kafka消息数据
首先，目前的日志模式es的索引都是统一的，并未使用日期进行区分，
所以直接在服务启动的时候去判断索引是否存在。某种程度下可以减少一次流程判断压力。
```

**java**

```
package com.kingtsoft.pangu.frame.log.server.biz.config;

import com.kingtsoft.pangu.data.es.IndexService;
import com.kingtsoft.pangu.frame.log.server.biz.listener.AnnotationLogListener;
import com.kingtsoft.pangu.frame.log.server.biz.listener.DbLogListener;
import com.kingtsoft.pangu.frame.log.server.biz.listener.NormalLogListener;
import lombok.SneakyThrows;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.ApplicationArguments;
import org.springframework.boot.ApplicationRunner;
import org.springframework.context.annotation.Configuration;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Slf4j
@Configuration
public class  EsLogStartupConfigimplementsApplicationRunner {

private final   IndexService indexService;

publicEsLogStartupConfig(IndexService indexService) {
this.indexService = indexService;
    }

    @Override
    @SneakyThrows
private void   run(ApplicationArguments args) {
// 持久层索引
if (!indexService.indexExists(DbLogListener.LOG_IDX)) {
            indexService.createIndex(DbLogListener.LOG_IDX);
        }
// 注解日志层索引
if (!indexService.indexExists(AnnotationLogListener.LOG_IDX)) {
            indexService.createIndex(AnnotationLogListener.LOG_IDX);
        }
// 常规日志记录索引
if (!indexService.indexExists(NormalLogListener.LOG_IDX)) {
            indexService.createIndex(NormalLogListener.LOG_IDX);
        }
    }
}
```

```
    然后对消息进行监听，然后创建一个setCreateTimelong ，主要是创建时间的时间点，
这样ES存储可以更轻松的进行比较排序。
```

**java**

```
package com.kingtsoft.pangu.frame.log.server.biz.listener;

import com.alibaba.fastjson2.JSON;
import com.kingtsoft.pangu.data.es.DocumentService;
import com.kingtsoft.pangu.frame.log.common.constants.FrameLogConst;
import com.kingtsoft.pangu.frame.log.common.model.LogOperateMessage;
import com.kingtsoft.pangu.frame.log.server.biz.utils.LogMessageUtil;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

import java.time.ZoneOffset;
import java.util.Optional;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Slf4j
@Component
public class  AnnotationLogListener {

private final   DocumentService<LogOperateMessage> documentService;

publicstaticfinal String LOG_IDX = FrameLogConst.LogIndex.LOG_ANNOTATION_IDX;

publicAnnotationLogListener(DocumentService<LogOperateMessage> documentService) {
this.documentService = documentService;
    }

    @KafkaListener(id="annotationLogListener", topics= {FrameLogConst.LOG_TOPIC_ANNOTATION}, groupId="mainGroup")
private void   annotationLogListener(ConsumerRecord<String, String> record) {
        Optional<String> message = Optional.ofNullable(record.value());
        String key = LogMessageUtil.getLogKey(record.key());

if (message.isEmpty()) {
            log.info("日志数据为空！ ");
return;
        }

try {
doAnnotationLogSave(message.get(), key);
        } catch (Exception e) {
            e.printStackTrace();
            log.error("日志数据保存失败！");
        }
    }

private void  doAnnotationLogSave(String msg, String key) {
        LogOperateMessage logOperateMessage = JSON.parseObject(msg, LogOperateMessage.class);
        logOperateMessage.setCreateTimelong (logOperateMessage.getCreateTime().toEpochSecond(ZoneOffset.of("+8")));
        documentService.saveOrUpdateDocument(LOG_IDX, key, logOperateMessage);
    }
}
```

```
    剩下的内容则为ES模块的各类CRUD操作
```

**java**

```
package com.kingtsoft.pangu.frame.log.server.biz.service;

/**
 * Title: <br>
 * Description: <br>
 * Company: KingTang <br>
 *
 * @author 金炀
 * @version 1.0
 */
@Service
public class  EsLogService {

private final   ElasticsearchClient elasticsearchClient;

private final   ElasticsearchAsyncClient elasticsearchAsyncClient;

publicEsLogService(ElasticsearchClient elasticsearchClient,
                        ElasticsearchAsyncClient elasticsearchAsyncClient) {
this.elasticsearchClient = elasticsearchClient;
this.elasticsearchAsyncClient = elasticsearchAsyncClient;
    }

    /**
     * 查询日志数据
     *
     * @paramesLogRequest 过滤条件
     * @return 日志数据
     * @author 金炀
     */
public JSONObject getEsLogData(EsLogRequest esLogRequest) {
if (esLogRequest.getLogClassify().equals(FrameLogConst.LogClassify.LOG_OPERATION)) {
            List<LogOperateMessage> data =getOperationEsLogData(esLogRequest);
long  total =getOperationEsLogTotal(esLogRequest);
returnnewJSONObject().fluentPut("data", data).fluentPut("total", total);
        } else {
            List<DbOperateLog> data =getDbEsLogData(esLogRequest);
long  total =getDbEsLogTotal(esLogRequest);
returnnewJSONObject().fluentPut("data", data).fluentPut("total", total);
        }
    }

    /**
     * 获取操作日志数据
     *
     * @paramesLogRequest 过滤条件
     * @return 操作日志数据
     * @author 金炀
     */
// 分页scroll
    @SneakyThrows
private List<LogOperateMessage> getOperationEsLogData(EsLogRequest esLogRequest) {
        SearchResponse<LogOperateMessage> response =getOperationEsLogDocData(esLogRequest);

        List<LogOperateMessage> operateMessages =new ArrayList<>();
for (Hit<LogOperateMessage> hit : response.hits().hits()) {
            operateMessages.add(hit.source());
        }

return operateMessages;
    }

    /**
     * 查询操作日志总数
     *
     * @paramesLogRequest 过滤条件
     * @return 操作日志总数
     * @author 金炀
     */
privatelong getOperationEsLogTotal(EsLogRequest esLogRequest) {
        CountResponse response =getOperationEsLogCountData(esLogRequest);
return response.count();
    }

    /**
     * 获取操作日志信息
     *
     * @paramesLogRequest 过滤条件
     * @return 操作日志信息
     * @author 金炀
     */
    @SneakyThrows
private SearchResponse<LogOperateMessage> getOperationEsLogDocData(EsLogRequest esLogRequest) {
return elasticsearchClient.search(
                builder -> {
                    builder.index(FrameLogConst.LogIndex.LOG_ANNOTATION_IDX)
                            .index(FrameLogConst.LogIndex.LOG_NORMAL_LOG)
                            .sort(sortBuilder -> sortBuilder.field(f -> f.order(SortOrder.Desc).field("createTimelong ")))
                            .highlight(getEsOperationHighlight(esLogRequest))
                            .query(getEsOperationQuery(esLogRequest))
                            .timeout("3s");

                    PgEsUtil.modifyPageData(esLogRequest, builder);
return builder;
                }, LogOperateMessage.class);
    }

    /**
     * 获取操作日志总数
     *
     * @paramesLogRequest 过滤条件
     * @return 操作日志总数
     * @author 金炀
     */
    @SneakyThrows
private CountResponse getOperationEsLogCountData(EsLogRequest esLogRequest) {
return elasticsearchClient.count(
                builder -> builder.index(FrameLogConst.LogIndex.LOG_ANNOTATION_IDX)
                        .index(FrameLogConst.LogIndex.LOG_NORMAL_LOG)
                        .query(getEsOperationQuery(esLogRequest))
        );
    }

    /**
     * 获取高亮执行信息
     *
     * @paramesLogRequest 过滤条件
     * @return 高亮执行信息
     * @author 金炀
     */
private Highlight getEsOperationHighlight(EsLogRequest esLogRequest) {
        Map<String, HighlightField> map =new HashMap<>();

if (StringUtils.hasText(esLogRequest.getSearchStr())) {
            map.put("operateName", getHighlightBuild(esLogRequest.getSearchStr()));
            map.put("operateContent", getHighlightBuild(esLogRequest.getSearchStr()));
        }
if (StringUtils.hasText(esLogRequest.getLogPosition())) {
            map.put("className", getHighlightBuild(esLogRequest.getLogPosition()));
            map.put("logPosition", getHighlightBuild(esLogRequest.getLogPosition()));
        }

return Highlight.of(h -> h.highlightQuery(getEsOperationQuery(esLogRequest)).fields(map));
    }

    /**
     * 获取操作日志总查询信息
     *
     * @paramesLogRequest 过滤条件
     * @return 操作日志总查询信息
     * @author 金炀
     */
private Query getEsOperationQuery(EsLogRequest esLogRequest) {
        List<Query> mustQueryList =new ArrayList<>();

if (StringUtils.hasText(esLogRequest.getSearchStr())) {
            mustQueryList.add(
                    Query.of(s -> s.bool(
                            v -> v.should(Query.of(q -> q.match(mf -> mf.field("operateName").analyzer("ik_max_word").query(qf -> qf.stringValue(esLogRequest.getSearchStr())))))
                                    .should(Query.of(q -> q.match(mf -> mf.field("operateContent").analyzer("ik_max_word").query(qf -> qf.stringValue(esLogRequest.getSearchStr())))))
                    ))
            );
        }
if (StringUtils.hasText(esLogRequest.getLogPosition())) {
            mustQueryList.add(
                    Query.of(s -> s.bool(
                            v -> v.should(Query.of(q -> q.match(mf -> mf.field("className").analyzer("ik_max_word").query(qf -> qf.stringValue(esLogRequest.getLogPosition())))))
                                    .should(Query.of(q -> q.match(mf -> mf.field("logPosition").analyzer("ik_max_word").query(qf -> qf.stringValue(esLogRequest.getLogPosition())))))
                    ))
            );
        }

buildWithCondition(esLogRequest, mustQueryList);
return QueryBuilders.bool(b -> b.must(mustQueryList));
    }

    /**
     * 获取数据库日志信息
     *
     * @paramesLogRequest 过滤条件
     * @return 数据库日志信息
     * @author 金炀
     */
    @SneakyThrows
private List<DbOperateLog> getDbEsLogData(EsLogRequest esLogRequest) {
        SearchResponse<DbOperateLog> response =getDbEsLogDocData(esLogRequest);

        List<DbOperateLog> operateLogs =new ArrayList<>();
for (Hit<DbOperateLog> hit : response.hits().hits()) {
            operateLogs.add(hit.source());
        }

return operateLogs;
    }

    /**
     * 获取数据库日志数据
     *
     * @paramesLogRequest 过滤条件
     * @return 数据库日志数据
     * @author 金炀
     */
    @SneakyThrows
private SearchResponse<DbOperateLog> getDbEsLogDocData(EsLogRequest esLogRequest) {
return elasticsearchClient.search(
                builder -> {
                    builder.index(FrameLogConst.LogIndex.LOG_DB_LOG)
                            .sort(sortBuilder -> sortBuilder.field(f -> f.order(SortOrder.Desc).field("createTimelong ")))
                            .highlight(getEsDbHighlight(esLogRequest))
                            .query(getEsDbQuery(esLogRequest))
                            .timeout("3s");

                    PgEsUtil.modifyPageData(esLogRequest, builder);
return builder;
                }, DbOperateLog.class);
    }

    /**
     * 获取高亮执行信息
     *
     * @paramesLogRequest 过滤条件
     * @return 高亮执行信息
     * @author 金炀
     */
private Highlight getEsDbHighlight(EsLogRequest esLogRequest) {
        Map<String, HighlightField> map =new HashMap<>();

if (StringUtils.hasText(esLogRequest.getSearchStr())) {
            map.put("execSql", getHighlightBuild(esLogRequest.getSearchStr()));
            map.put("operateContent", getHighlightBuild(esLogRequest.getSearchStr()));
        }
if (StringUtils.hasText(esLogRequest.getLogPosition())) {
            map.put("logPosition", getHighlightBuild(esLogRequest.getLogPosition()));
        }

if (StringUtils.hasText(esLogRequest.getSourceStr())) {
            map.put("userId", getHighlightBuild(esLogRequest.getSourceStr()));
            map.put("terminal", getHighlightBuild(esLogRequest.getSourceStr()));
        }

return Highlight.of(h -> h.highlightQuery(getEsDbQuery(esLogRequest)).fields(map));
    }

    /**
     * 封装统一的高亮样式
     *
     * @paramstr 高亮文字
     * @return 高亮
     * @author 金炀
     */
private HighlightField getHighlightBuild(String str) {
returnnew HighlightField.Builder()
                .matchedFields(str)
                .requireFieldMatch(false)
                .preTags("<sp>")
                .postTags("</sp>")
                .build();
    }

    /**
     * 获取数据库日志总数
     *
     * @paramesLogRequest 过滤条件
     * @return 数据库日志总数
     * @author 金炀
     */
privatelong getDbEsLogTotal(EsLogRequest esLogRequest) {
        CountResponse response =getDbEsLogCountData(esLogRequest);
return response.count();
    }

    /**
     * 获取数据库日志总数结构体
     *
     * @paramesLogRequest 过滤条件
     * @return 数据库日志总数结构体
     * @author 金炀
     */
    @SneakyThrows
private CountResponse getDbEsLogCountData(EsLogRequest esLogRequest) {
return elasticsearchClient.count(
                builder -> builder.index(FrameLogConst.LogIndex.LOG_DB_LOG).query(getEsDbQuery(esLogRequest))
        );
    }

    /**
     * 获取数据库日志总条件
     *
     * @paramesLogRequest 过滤条件
     * @return 数据库日志总条件
     * @author 金炀
     */
private Query getEsDbQuery(EsLogRequest esLogRequest) {
        List<Query> mustQueryList =new ArrayList<>();

if (StringUtils.hasText(esLogRequest.getSearchStr())) {
            mustQueryList.add(
                    Query.of(s -> s.bool(
                            v -> v.should(Query.of(q -> q.match(mf -> mf.field("execSql").query(qf -> qf.stringValue(esLogRequest.getSearchStr())))))
                                    .should(Query.of(q -> q.match(mf -> mf.field("operateContent").query(qf -> qf.stringValue(esLogRequest.getSearchStr())))))
                    ))
            );
        }
if (StringUtils.hasText(esLogRequest.getLogPosition())) {
            mustQueryList.add(
                    Query.of(s -> s.match(mf -> mf.field("logPosition").query(qf -> qf.stringValue(esLogRequest.getLogPosition()))))
            );
        }
if (StringUtils.hasText(esLogRequest.getSourceStr())) {
            mustQueryList.add(
                    Query.of(s -> s.bool(
                            v -> v.should(Query.of(q -> q.match(mf -> mf.field("userId").query(qf -> qf.stringValue(esLogRequest.getSourceStr())))))
                                    .should(Query.of(q -> q.match(mf -> mf.field("terminal").query(qf -> qf.stringValue(esLogRequest.getSourceStr())))))
                    ))
            );
        }

buildWithCondition(esLogRequest, mustQueryList);
return QueryBuilders.bool(b -> b.must(mustQueryList));
    }


    /**
     * 构建查询结构体
     *
     * @paramesLogRequest 过滤条件
     * @parammustQueryList 执行条件集合
     * @author 金炀
     */
private void  buildWithCondition(EsLogRequest esLogRequest, List<Query> mustQueryList) {
if (StringUtils.hasText(esLogRequest.getModule())) {
            mustQueryList.add(
                    Query.of(q -> q.match(
                            mf -> mf.field("module").query(qf -> qf.stringValue(esLogRequest.getModule()))
                    ))
            );
        }
if (StringUtils.hasText(esLogRequest.getTraceId())) {
            mustQueryList.add(
                    Query.of(q -> q.wildcard(
                            mf -> mf.field("traceId").value(esLogRequest.getTraceId() +"*")))
            );
        }
if (StringUtils.hasText(esLogRequest.getTerminal())) {
            mustQueryList.add(
                    Query.of(q -> q.match(
                            mf -> mf.field("terminal").query(qf -> qf.stringValue(esLogRequest.getTerminal()))
                    ))
            );
        }
    }

    /**
     * 删除日志数据(一般不开放)
     *
     * @paramesLogRequest 过滤条件
     * @author 金炀
     */
    @SneakyThrows
private void   delEsLogData(EsLogRequest esLogRequest) {
        SearchResponse<LogOperateMessage> response =getOperationEsLogDocData(esLogRequest);

for (Hit<LogOperateMessage> hit : response.hits().hits()) {
            elasticsearchClient.delete(d -> d.index(hit.index()).id(hit.id()));
        }

        SearchResponse<DbOperateLog> response2 =getDbEsLogDocData(esLogRequest);
for (Hit<DbOperateLog> hit : response2.hits().hits()) {
            elasticsearchClient.delete(d -> d.index(hit.index()).id(hit.id()));
        }
    }
}
```
